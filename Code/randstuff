#https://github.com/Sousannah/hand-tracking-using-mediapipe
import cv2
import mediapipe as mp
import time
import numpy as np
from pythonosc.udp_client import SimpleUDPClient
import random
import time
from pathlib import Path

pTime = 0
cTime = 0

lmkNames = ["wrist", "thumb_cmc", "thumb_mcp", "thumb_ip", "thumb_tip",
            "index_finger_mcp", "index_finger_pip", "index_finger_dip", "index_finger_tip",
            "middle_finger_mcp", "middle_finger_pip", "middle_finger_dip", "middle_finger_tip",
            "ring_finger_mcp", "ring_finger_pip", "ring_finger_dip", "ring_finger_tip",
            "pinky_mcp", "pinky_pip", "pinky_dip", "pinky_tip"]

# Set the IP and port (MaxMSP listens on port 7400)
ip = "127.0.0.1"
port = 7400

BaseOptions = mp.tasks.BaseOptions
GestureRecognizer = mp.tasks.vision.GestureRecognizer
GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions
GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult
VisionRunningMode = mp.tasks.vision.RunningMode

awaitResult = True

# Create a gesture recognizer instance with the live stream mode:
def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):
    #print('gesture recognition result: {}'.format(result))
    client.send_message("/numHands", [len(result.hand_world_landmarks)])
    #todo- gesture array
    #for gesture in result.gestures:
    #    print(gesture.categoryName)

    for handLms in result.hand_landmarks:
        id = 0
        for landmark in handLms:
            #print(landmark.x)
            client.send_message("/" + lmkNames[id], [int(landmark.x * 1000), int(landmark.y * 1000), int(landmark.z * 1000)])
            id = id + 1

    #print (fps)

    #output_ndarray = output_image.numpy_view()
    #outImgOpencv = np.zeros((100, 100, 3), np.uint8)
    #global outimg
    #outimg = np.copy(output_ndarray)

    global awaitResult
    awaitResult = False

with open(str(Path("./gesture_recognizer.task").resolve()), 'rb') as file:
    model_data = file.read()

    options = GestureRecognizerOptions(
        base_options=BaseOptions(model_asset_buffer=model_data),
        running_mode=VisionRunningMode.LIVE_STREAM,
        result_callback=print_result)
    with GestureRecognizer.create_from_options(options) as recognizer:

        cap = cv2.VideoCapture(0)

        client = SimpleUDPClient(ip, port) # Create an OSC client

        startTime = time.perf_counter_ns() // 1000000

        while True:
            success, img = cap.read()
            currentFrameTime = (time.perf_counter_ns() // 1000000) - startTime
            #print(currentFrameTime)
            img = cv2.flip(img, 1)
            imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)
            awaitResult = True
            recognizer.recognize_async(mp_image, currentFrameTime)
            #while(awaitResult):
            #    pass

            cTime = time.time()
            fps = 1 / (cTime - pTime)
            pTime = cTime

            h, w, c = img.shape
            outImgOpencv = np.zeros((h, w, 3), np.uint8)
            cv2.putText(outImgOpencv, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3)
            cv2.imshow("Image", outImgOpencv)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break

cap.release()
cv2.destroyAllWindows()
